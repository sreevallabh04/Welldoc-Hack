# ü§ñ Local LLM Setup Guide for SMIT Automation Framework

## Overview

The SMIT Automation Framework now supports **local LLM integration** for AI-powered code generation. This guide shows you how to set up and use local LLMs with the framework.

## üéØ Supported Local LLM Platforms

### 1. **Ollama** (Recommended)
- **Download**: https://ollama.ai/
- **Models**: CodeLlama, Llama2, Mistral, Phi
- **Default Endpoint**: `http://localhost:11434/api/generate`

### 2. **LM Studio**
- **Download**: https://lmstudio.ai/
- **Models**: Various open-source models
- **Default Endpoint**: `http://localhost:1234/v1/chat/completions`

### 3. **Ollama Web UI**
- **GitHub**: https://github.com/open-webui/open-webui
- **Models**: Same as Ollama
- **Default Endpoint**: `http://localhost:11434/api/generate`

## üöÄ Quick Setup with Ollama

### Step 1: Install Ollama
```bash
# Windows (PowerShell)
winget install Ollama.Ollama

# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.ai/install.sh | sh
```

### Step 2: Download Code Model
```bash
# Download CodeLlama (recommended for code generation)
ollama pull codellama

# Alternative models
ollama pull llama2
ollama pull mistral
ollama pull phi
```

### Step 3: Start Ollama Service
```bash
# Start Ollama service
ollama serve

# In another terminal, verify it's running
curl http://localhost:11434/api/tags
```

### Step 4: Test the Framework
```bash
# Run the generator with LLM integration
mvn exec:java -Dexec.mainClass="generator.GenerateFromExcel"
```

## üîß Configuration Options

### Environment Variables
```bash
# Set custom LLM endpoint
export LLM_ENDPOINT="http://localhost:11434/api/generate"

# Set custom model
export LLM_MODEL="codellama"

# Set custom port
export LLM_PORT="11434"
```

### Java System Properties
```bash
# Run with custom configuration
mvn exec:java -Dexec.mainClass="generator.GenerateFromExcel" \
  -Dllm.endpoint="http://localhost:11434/api/generate" \
  -Dllm.model="codellama"
```

## üìù LLM Integration Features

### 1. **Automatic Detection**
The framework automatically detects if a local LLM is available:
```
ü§ñ Using local LLM for code generation...
‚ö†Ô∏è  Local LLM not available, using template-based generation...
```

### 2. **Intelligent Prompting**
The framework builds sophisticated prompts for the LLM:
```
Generate a Java Page Object Model class for Selenium automation.

Requirements:
- Class name: LoginPage
- Extends BasePage class
- Use Selenium WebDriver
- Include proper Javadoc comments
- Add TODO comments for locator verification

Test cases for this page:
- Login to SMIT Portal
  Steps: Navigate to URL, Enter username, Enter password, Click Login
  Test Data: Username: welldocsu, Password: welldoc123

Generate locators and methods based on the test cases above.
```

### 3. **Fallback Mechanism**
If the LLM is unavailable, the framework falls back to template-based generation:
```java
// Fallback code generated when local LLM is unavailable
// TODO: Update this code manually or ensure LLM is running
```

## üéØ LLM-Generated Code Examples

### Generated Page Object Model
```java
package pages;

import org.openqa.selenium.By;
import org.openqa.selenium.WebDriver;

/**
 * Page Object Model for SMIT Portal Login Page
 * Generated by local LLM (CodeLlama)
 */
public class LoginPage extends BasePage {
    
    // LLM-generated locators based on test cases
    private final By usernameField = By.id("username");
    private final By passwordField = By.id("password");
    private final By loginButton = By.id("login");
    private final By loginForm = By.id("loginForm");
    
    public LoginPage(WebDriver driver) {
        super(driver);
    }
    
    /**
     * Opens the SMIT Portal login page
     */
    public void open() {
        open("https://azqa21-dsm.testwd.com/SMITPortal/Guest/Login.htm");
    }
    
    /**
     * Performs login action
     */
    public void login(String username, String password) {
        type(usernameField, username);
        type(passwordField, password);
        click(loginButton);
    }
    
    /**
     * Checks if user is logged in
     */
    public boolean isLoggedIn() {
        return !getCurrentUrl().contains("Login");
    }
}
```

### Generated Test Method
```java
/**
 * Test Case: TC_SMIT_01 - Login to SMIT Portal
 * Generated by local LLM
 */
@Test(description = "Login to SMIT Portal", priority = 1)
public void loginPortal() {
    try {
        LoginPage loginPage = new LoginPage(driver);
        loginPage.open();
        loginPage.login("welldocsu", "welldoc123");
        Assert.assertTrue(loginPage.isLoggedIn(), "User should be successfully logged in");
        
        System.out.println("TC_SMIT_01 - Test completed successfully");
    } catch (Exception e) {
        System.err.println("TC_SMIT_01 - Test failed: " + e.getMessage());
        Assert.fail("Test failed: " + e.getMessage());
    }
}
```

## üîç Troubleshooting

### Common Issues

#### 1. **LLM Not Available**
```
‚ùå Local LLM not available: Connection refused
```
**Solution**: Ensure Ollama is running (`ollama serve`)

#### 2. **Model Not Found**
```
‚ùå LLM generation failed: model not found
```
**Solution**: Download the model (`ollama pull codellama`)

#### 3. **Connection Timeout**
```
‚ùå LLM generation failed: Connection timeout
```
**Solution**: Check firewall settings and port availability

#### 4. **Empty Response**
```
‚ùå LLM generation failed: Empty response from LLM
```
**Solution**: Try a different model or restart Ollama service

### Debug Mode
```bash
# Enable debug logging
mvn exec:java -Dexec.mainClass="generator.GenerateFromExcel" -Ddebug=true
```

## üéØ Best Practices

### 1. **Model Selection**
- **CodeLlama**: Best for Java code generation
- **Llama2**: Good general-purpose model
- **Mistral**: Fast and efficient
- **Phi**: Lightweight option

### 2. **Prompt Engineering**
The framework automatically builds effective prompts, but you can customize them in `LLMService.java`:
```java
request.put("options", Map.of(
    "temperature", 0.1,    // Low temperature for consistent code
    "top_p", 0.9,         // Focus on most likely tokens
    "max_tokens", 2000    // Sufficient for complete classes
));
```

### 3. **Performance Optimization**
- Use GPU acceleration if available
- Keep models loaded in memory
- Use appropriate model size for your hardware

## üöÄ Advanced Configuration

### Custom LLM Endpoint
```java
// Use custom LLM service
LLMService llmService = new LLMService("http://localhost:1234/v1/chat/completions", "custom-model");
```

### Multiple Model Support
```java
// Switch between models
llmService.setModel("codellama");  // For code generation
llmService.setModel("llama2");     // For general tasks
```

## üìä Performance Comparison

| Method | Speed | Quality | Flexibility |
|--------|-------|---------|-------------|
| Template-based | ‚ö°‚ö°‚ö° Fast | ‚≠ê‚≠ê Good | ‚≠ê Limited |
| LLM-generated | ‚ö°‚ö° Medium | ‚≠ê‚≠ê‚≠ê Excellent | ‚≠ê‚≠ê‚≠ê High |

## üéâ Conclusion

The local LLM integration transforms the SMIT Automation Framework from a template-based generator into a true **AI-powered automation framework generator**. This meets the hackathon requirement for using open-source LLMs running locally.

**Benefits**:
- ‚úÖ **Dynamic code generation** based on test cases
- ‚úÖ **Intelligent locator generation** 
- ‚úÖ **Context-aware method creation**
- ‚úÖ **Fallback to templates** when LLM unavailable
- ‚úÖ **Complete local processing** (no external APIs)

**Ready to use with any local LLM platform!** ü§ñ
